{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2', 'ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT', 'MIXTRAL_8X7B_INSTRUCT_V01_Q']\n"
     ]
    }
   ],
   "source": [
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=ModelTypes.GRANITE_13B_CHAT_V2.value,\n",
    "    url=st.secrets[\"WATSONX_URL\"],\n",
    "    apikey=st.secrets[\"IBM_CLOUD_API_KEY\"],\n",
    "    project_id=st.secrets[\"WATSONX_PROJECT_ID\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-03-27)\n",
      "Status code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\"}],\"trace\":\"15b21966d0a881d7c03df4690af00ad6\",\"status_code\":403}\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-03-27)\nStatus code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\"}],\"trace\":\"15b21966d0a881d7c03df4690af00ad6\",\"status_code\":403}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m watsonx_llm\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mwho is the ceo of neo4j?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    245\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    246\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 248\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    249\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    250\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    251\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    252\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    253\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    254\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    255\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    256\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    258\u001b[0m         \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m         \u001b[39m.\u001b[39mtext\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    567\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    568\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    735\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    736\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[0;32m--> 748\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    749\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    750\u001b[0m     )\n\u001b[1;32m    751\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    605\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    607\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    608\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_core/language_models/llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    590\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    591\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 593\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    594\u001b[0m                 prompts,\n\u001b[1;32m    595\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    596\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    597\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    598\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    599\u001b[0m             )\n\u001b[1;32m    600\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    601\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/langchain_ibm/llms.py:385\u001b[0m, in \u001b[0;36mWatsonxLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[generation]])\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwatsonx_model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    386\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompts, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    387\u001b[0m     )\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_llm_result(response)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py:228\u001b[0m, in \u001b[0;36mModelInference.generate\u001b[0;34m(self, prompt, params, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit, async_mode)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m concurrency_limit \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m \u001b[39mor\u001b[39;00m concurrency_limit \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m ParamOutOfRange(param_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconcurrency_limit\u001b[39m\u001b[39m'\u001b[39m, value\u001b[39m=\u001b[39mconcurrency_limit, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference\u001b[39m.\u001b[39;49mgenerate(prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    229\u001b[0m                                 params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    230\u001b[0m                                 guardrails\u001b[39m=\u001b[39;49mguardrails,\n\u001b[1;32m    231\u001b[0m                                 guardrails_hap_params\u001b[39m=\u001b[39;49mguardrails_hap_params,\n\u001b[1;32m    232\u001b[0m                                 guardrails_pii_params\u001b[39m=\u001b[39;49mguardrails_pii_params,\n\u001b[1;32m    233\u001b[0m                                 concurrency_limit\u001b[39m=\u001b[39;49mconcurrency_limit,\n\u001b[1;32m    234\u001b[0m                                 async_mode\u001b[39m=\u001b[39;49masync_mode)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py:89\u001b[0m, in \u001b[0;36mFMModelInference.generate\u001b[0;34m(self, prompt, params, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit, async_mode)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_url_async(prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m     82\u001b[0m                                          params\u001b[39m=\u001b[39mparams \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams,\n\u001b[1;32m     83\u001b[0m                                          generate_url\u001b[39m=\u001b[39mgenerate_text_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          guardrails_pii_params\u001b[39m=\u001b[39mguardrails_pii_params,\n\u001b[1;32m     87\u001b[0m                                          concurrency_limit\u001b[39m=\u001b[39mconcurrency_limit)\n\u001b[1;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_url(prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     90\u001b[0m                                    params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m     91\u001b[0m                                    generate_url\u001b[39m=\u001b[39;49mgenerate_text_url,\n\u001b[1;32m     92\u001b[0m                                    guardrails\u001b[39m=\u001b[39;49mguardrails,\n\u001b[1;32m     93\u001b[0m                                    guardrails_hap_params\u001b[39m=\u001b[39;49mguardrails_hap_params,\n\u001b[1;32m     94\u001b[0m                                    guardrails_pii_params\u001b[39m=\u001b[39;49mguardrails_pii_params,\n\u001b[1;32m     95\u001b[0m                                    concurrency_limit\u001b[39m=\u001b[39;49mconcurrency_limit)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:128\u001b[0m, in \u001b[0;36mBaseModelInference._generate_with_url\u001b[0;34m(self, prompt, params, generate_url, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompt) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m concurrency_limit:\n\u001b[1;32m    127\u001b[0m     \u001b[39mwith\u001b[39;00m ThreadPoolExecutor() \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 128\u001b[0m         response_batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    129\u001b[0m             executor\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_url, prompt, [params] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(prompt),\n\u001b[1;32m    130\u001b[0m                          [generate_url] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(prompt), [guardrails] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(prompt),\n\u001b[1;32m    131\u001b[0m                          [guardrails_hap_params] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(prompt), [guardrails_pii_params] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(prompt))\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m     generated_responses\u001b[39m.\u001b[39mextend(response_batch)\n\u001b[1;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:148\u001b[0m, in \u001b[0;36mBaseModelInference._generate_with_url\u001b[0;34m(self, prompt, params, generate_url, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m generated_responses\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_inference_payload(prompt,\n\u001b[1;32m    149\u001b[0m                                         params,\n\u001b[1;32m    150\u001b[0m                                         generate_url,\n\u001b[1;32m    151\u001b[0m                                         guardrails,\n\u001b[1;32m    152\u001b[0m                                         guardrails_hap_params,\n\u001b[1;32m    153\u001b[0m                                         guardrails_pii_params)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:110\u001b[0m, in \u001b[0;36mBaseModelInference._send_inference_payload\u001b[0;34m(self, prompt, params, generate_url, guardrails, guardrails_hap_params, guardrails_pii_params)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_response(\u001b[39m200\u001b[39;49m, \u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgenerate\u001b[39;49m\u001b[39m'\u001b[39;49m, response_scoring)\n",
      "File \u001b[0;32m~/mambaforge/envs/neo_llm/lib/python3.11/site-packages/ibm_watson_machine_learning/wml_resource.py:64\u001b[0m, in \u001b[0;36mWMLResource._handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m ApiRequestFailure(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailure during \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(operationName), response)\n",
      "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-03-27)\nStatus code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\"}],\"trace\":\"15b21966d0a881d7c03df4690af00ad6\",\"status_code\":403}"
     ]
    }
   ],
   "source": [
    "watsonx_llm.invoke(\"who is the ceo of neo4j?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
